{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e810749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 4.2.2\n",
      "Apache Spark version: 3.3.1\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start(gpu=True) \n",
    "# sparknlp.start(gpu=True) >> for training on GPUfrom sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f12d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d44868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9fa35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------+\n",
      "|                     description|category|\n",
      "+--------------------------------+--------+\n",
      "|           i want to make coffee|      DA|\n",
      "|                      where am i|      LR|\n",
      "|              what can i do here|      AR|\n",
      "|                    prepare meal|      DA|\n",
      "|                use refrigerator|      DA|\n",
      "|                         use fan|      DA|\n",
      "|                        use oven|      DA|\n",
      "|                       use stove|      DA|\n",
      "|     i would like to wash sheets|      DA|\n",
      "|i would like to watch television|      DA|\n",
      "+--------------------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"text_train.csv\")\n",
    "trainDataset.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25422e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------+\n",
      "|                     description|category|\n",
      "+--------------------------------+--------+\n",
      "|           i want to make coffee|      DA|\n",
      "|                      where am i|      LR|\n",
      "|              what can i do here|      AR|\n",
      "|                    prepare meal|      DA|\n",
      "|                use refrigerator|      DA|\n",
      "|                         use fan|      DA|\n",
      "|                        use oven|      DA|\n",
      "|                       use stove|      DA|\n",
      "|     i would like to wash sheets|      DA|\n",
      "|i would like to watch television|      DA|\n",
      "+--------------------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"text_test.csv\")\n",
    "testDataset.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69940fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|      VC|   46|\n",
      "|      DA|   29|\n",
      "|      CC|   26|\n",
      "|      SD|   23|\n",
      "|      LR|   21|\n",
      "|      AR|   20|\n",
      "|      SN|   20|\n",
      "|     NAV|   18|\n",
      "|      NA|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "trainDataset.groupBy(\"category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1ded33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|      VC|   46|\n",
      "|      DA|   29|\n",
      "|      CC|   26|\n",
      "|      SD|   23|\n",
      "|      LR|   21|\n",
      "|      AR|   20|\n",
      "|      SN|   20|\n",
      "|     NAV|   18|\n",
      "|      NA|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"text_test.csv\")\n",
    "testDataset.groupBy(\"category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e763ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_small_bert_L8_512 download started this may take some time.\n",
      "Approximate size to download 149.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# actual content is inside description column\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"description\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "# we can also use sentence detector here \n",
    "# if we want to train on and get predictions for each sentence# downloading pretrained embeddings\n",
    "use = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    "# the classes/labels/categories are in category column\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"class\")\\\n",
    "    .setLabelColumn(\"category\")\\\n",
    "    .setMaxEpochs(200)\\\n",
    "    .setBatchSize(32)\\\n",
    "    .setRandomSeed(0)\\\n",
    "    .setVerbose(1)\\\n",
    "    .setValidationSplit(0.2)\\\n",
    "    .setRandomSeed(0)\\\n",
    "    .setEnableOutputLogs(True)\\\n",
    "    .setOutputLogsPath('classifer_logs')\\\n",
    "\n",
    "use_clf_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        classsifierdl\n",
    "    ])\n",
    "\n",
    "use_pipelineModel = use_clf_pipeline.fit(trainDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea4a3607",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-11439f2e50b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_pipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "df = use_pipelineModel.transform(testDataset).select(\"category\",\"description\",\"class\").toPandas()\n",
    "df['result']=df['class'].apply(lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b50095a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pipelineModel.stages[2].write().overwrite().save('Text_Classification_Bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56923ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions =  use_pipelineModel.transform(testDataset)\n",
    "#predictions.select(\"category\", \"text\", \"class.result\").show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ba9cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_small_bert_L8_512 download started this may take some time.\n",
      "Approximate size to download 149.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# actual content is inside description column\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"description\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "# we can also use sentence detector here \n",
    "# if we want to train on and get predictions for each sentence# downloading pretrained embeddings\n",
    "use = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    "# the classes/labels/categories are in category column\n",
    "loaded_ner_model = ClassifierDLModel.load(\"Text_Classification_Bert\")\\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"class\")\n",
    "\n",
    "use_clf_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        loaded_ner_model\n",
    "    ])\n",
    "\n",
    "#use_pipelineModel = use_clf_pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d70458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': ['navigate to kitchen'], 'sentence_embeddings': ['navigate to kitchen'], 'class': ['NAV']}\n",
      "['NAV']\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import LightPipeline\n",
    "#clf_pipelineModel = use_clf_pipeline.fit(trainDataset)\n",
    "\n",
    "text = \"navigate to kitchen\"\n",
    "prediction_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "prediction_model = use_clf_pipeline.fit(prediction_data)\n",
    "light = LightPipeline(prediction_model)\n",
    "results = light.annotate(text)\n",
    "print(results)\n",
    "print(results['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c39d6145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC']\n",
      "['CC']\n",
      "['CC']\n",
      "['CC']\n",
      "['CC']\n",
      "['CC']\n",
      "['VC']\n",
      "['VC']\n",
      "['VC']\n",
      "['VC']\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter Testing Text\\n\")\n",
    "while(text != \"exit\"):\n",
    "    results = light.annotate(text)\n",
    "    print(results['class'])\n",
    "    text = input(\"Enter New Text\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8da94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2243226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f123c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e5299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544efb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b92a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d208a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a21197b13add4b6c4ce7cb24d3cb94b238afe3cb21fedcec323a08881ade42e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
