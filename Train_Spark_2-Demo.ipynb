{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6bc4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 16:35:39 WARN Utils: Your hostname, Vader resolves to a loopback address: 127.0.1.1; using 192.168.1.94 instead (on interface enp5s0f0)\n",
      "22/11/16 16:35:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/sjhjrok/anaconda3/envs/NLP/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sjhjrok/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sjhjrok/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp-gpu_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-90d52822-93ca-4636-ba2c-e549564626d3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp-gpu_2.12;4.2.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-gpu_2.12;0.4.3 in central\n",
      ":: resolution report :: resolve 204ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp-gpu_2.12;4.2.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-gpu_2.12;0.4.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-90d52822-93ca-4636-ba2c-e549564626d3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 16:35:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 4.2.2\n",
      "Apache Spark version: 3.3.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sparknlp\n",
    " \n",
    "spark = sparknlp.start(gpu = True) # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from sparknlp.common import *\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    " \n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    " \n",
    "print(\"Apache Spark version:\", spark.version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, './eng_custom.train')\n",
    "testing_data= CoNLL().readDataset(spark, './eng_custom.testa')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "#training_data.select(F.explode(F.arrays_zip('token.result', 'pos.result',  'label.result')).alias(\"cols\")) \\\n",
    "#.select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "#        F.expr(\"cols['1']\").alias(\"pos\"),\n",
    "#        F.expr(\"cols['2']\").alias(\"ner_label\")).show(truncate=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecff060",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ner_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# You can use any word embeddings you want (Glove, Elmo, Bert, custom etc.)\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "embeddings = WordEmbeddingsModel.pretrained('glove_100d')\\\n",
    "          .setInputCols([\"document\", \"token\"])\\\n",
    "          .setOutputCol(\"embeddings\")\n",
    "\n",
    "nerTagger = NerDLApproach()\\\n",
    "      .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "      .setLabelColumn(\"label\")\\\n",
    "      .setOutputCol(\"ner\")\\\n",
    "      .setMaxEpochs(500)\\\n",
    "      .setLr(0.003)\\\n",
    "      .setBatchSize(32)\\\n",
    "      .setRandomSeed(0)\\\n",
    "      .setVerbose(1)\\\n",
    "      .setValidationSplit(0.2)\\\n",
    "      .setEvaluationLogExtended(True) \\\n",
    "      .setEnableOutputLogs(True)\\\n",
    "      .setIncludeConfidence(True)\\\n",
    "      .setOutputLogsPath('ner_logs') # if not set, logs will be written to ~/annotator_logs\n",
    " #    .setGraphFolder('graphs') >> put your graph file (pb) under this folder if you are using a custom graph generated thru 4.1 NerDL-Graph.ipynb notebook\n",
    " #    .setEnableMemoryOptimizer() >> if you have a limited memory and a large conll file, you can set this True to train batch by batch \n",
    "    \n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols(['document', 'token', 'ner']) \\\n",
    "    .setOutputCol('ner_chunk')\n",
    "\n",
    "ner_pipeline = Pipeline(stages=[\n",
    "      embeddings,\n",
    "      nerTagger,\n",
    "      ner_converter\n",
    " ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ner_model = ner_pipeline.fit(training_data)\n",
    "ner_model.stages[1].write().overwrite().save('outputs/ner_wiki_glove100d_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ab57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#predictions = ner_model.transform(testing_data) \n",
    "#predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
    "#           .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "#                   F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
    "#                   F.expr(\"cols['2']\").alias(\"prediction\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    " \n",
    "#preds_df = predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
    "#                      .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "#                              F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
    "#                              F.expr(\"cols['2']\").alias(\"prediction\")).toPandas()\n",
    " \n",
    "#print(classification_report(preds_df['ground_truth'], preds_df['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil\n",
    "\n",
    "#shutil.make_archive(\"/home/sjhjrok/Documents/NLP/outputs/ner_wiki_glove100d_en\", 'zip', \"/home/sjhjrok/Documents/NLP/outputs/ner_wiki_glove100d_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"\"\"Navagate to kitchen\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ner_model = NerDLModel.load(\"outputs/ner_wiki_glove100d_en\")\\\n",
    "   .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "   .setOutputCol(\"ner\")\n",
    "\n",
    "load_ner_pipeline = Pipeline(stages=[\n",
    "      document_assembler,\n",
    "      tokenizer,\n",
    "      embeddings,\n",
    "      loaded_ner_model,\n",
    "      ner_converter\n",
    " ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93879e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import LightPipeline\n",
    "\n",
    "text = \"navigate to kitchen\"\n",
    "prediction_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "prediction_model = load_ner_pipeline.fit(prediction_data)\n",
    "light = LightPipeline(prediction_model)\n",
    "results = light.annotate(text)\n",
    "print(results)\n",
    "print(results['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "light.annotate(\"Navigate to kitchen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Enter Testing Text\\n\")\n",
    "while(text != \"exit\"):\n",
    "    results = light.annotate(text)\n",
    "    print(results['ner'])\n",
    "    text = input(\"Enter New Text\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ec59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty_df = spark.createDataFrame([['']]).toDF('text')\n",
    "#pipeline_model = ner_pipeline.fit(empty_df)\n",
    "#df = spark.createDataFrame(pd.DataFrame({'text': text_list}))\n",
    "#result = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#from sparknlp_display import NerVisualizer\n",
    "\n",
    "\n",
    "\n",
    "#result = predictions\n",
    "\n",
    "#NerVisualizer().display(\n",
    "#    result = result.collect()[0],\n",
    "#    label_col = 'ner_chunk',\n",
    "#    document_col = 'document'\n",
    "#)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166033e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bfae91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df42be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a75326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7770d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7580b42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fdd75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4caac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb755f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2805a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdf9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc37b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb86780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c570b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eae113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0ab4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce880a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c9163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab5d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85b504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb837a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb7d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
