{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e810749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 4.2.2\n",
      "Apache Spark version: 3.3.1\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start(gpu=True) \n",
    "# sparknlp.start(gpu=True) >> for training on GPUfrom sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f12d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41d44868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c9fa35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------+\n",
      "|                     description|category|\n",
      "+--------------------------------+--------+\n",
      "|           i want to make coffee|      DA|\n",
      "|                      where am i|      LR|\n",
      "|              what can i do here|      AR|\n",
      "|                    prepare meal|      DA|\n",
      "|                use refrigerator|      DA|\n",
      "|                         use fan|      DA|\n",
      "|                        use oven|      DA|\n",
      "|                       use stove|      DA|\n",
      "|     i would like to wash sheets|      DA|\n",
      "|i would like to watch television|      DA|\n",
      "+--------------------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"text_train.csv\")\n",
    "trainDataset.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25422e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------+\n",
      "|                     description|category|\n",
      "+--------------------------------+--------+\n",
      "|           i want to make coffee|      DA|\n",
      "|                      where am i|      LR|\n",
      "|              what can i do here|      AR|\n",
      "|                    prepare meal|      DA|\n",
      "|                use refrigerator|      DA|\n",
      "|                         use fan|      DA|\n",
      "|                        use oven|      DA|\n",
      "|                       use stove|      DA|\n",
      "|     i would like to wash sheets|      DA|\n",
      "|i would like to watch television|      DA|\n",
      "+--------------------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"text_test.csv\")\n",
    "testDataset.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69940fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|      VC|   46|\n",
      "|      DA|   29|\n",
      "|      CC|   26|\n",
      "|      SD|   23|\n",
      "|      LR|   21|\n",
      "|      AR|   20|\n",
      "|      SN|   20|\n",
      "|     NAV|   18|\n",
      "|      NA|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "trainDataset.groupBy(\"category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b1ded33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|      VC|   46|\n",
      "|      DA|   29|\n",
      "|      CC|   26|\n",
      "|      SD|   23|\n",
      "|      LR|   21|\n",
      "|      AR|   20|\n",
      "|      SN|   20|\n",
      "|     NAV|   18|\n",
      "|      NA|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"text_test.csv\")\n",
    "testDataset.groupBy(\"category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e763ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# actual content is inside description column\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"description\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "# we can also use sentence detector here \n",
    "# if we want to train on and get predictions for each sentence# downloading pretrained embeddings\n",
    "use = UniversalSentenceEncoder.pretrained()\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "# the classes/labels/categories are in category column\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"class\")\\\n",
    "    .setLabelColumn(\"category\")\\\n",
    "    .setMaxEpochs(500)\\\n",
    "    .setLr(0.01)\\\n",
    "    .setBatchSize(32)\\\n",
    "    .setRandomSeed(957)\\\n",
    "    .setVerbose(1)\\\n",
    "    .setEvaluationLogExtended(True) \\\n",
    "    .setEnableOutputLogs(True)\\\n",
    "    .setOutputLogsPath('classifer_logs')\\\n",
    "\n",
    "use_clf_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        classsifierdl\n",
    "    ])\n",
    "\n",
    "use_pipelineModel = use_clf_pipeline.fit(trainDataset)\n",
    "#.setValidationSplit(0.2)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b50095a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pipelineModel.stages[2].write().overwrite().save('Text_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56923ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions =  use_pipelineModel.transform(testDataset)\n",
    "#predictions.select(\"category\", \"text\", \"class.result\").show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ba9cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# actual content is inside description column\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"description\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "# we can also use sentence detector here \n",
    "# if we want to train on and get predictions for each sentence# downloading pretrained embeddings\n",
    "use = UniversalSentenceEncoder.pretrained()\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "# the classes/labels/categories are in category column\n",
    "loaded_ner_model = ClassifierDLModel.load(\"Text_Classification\")\\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"class\")\n",
    "\n",
    "use_clf_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        loaded_ner_model\n",
    "    ])\n",
    "\n",
    "#use_pipelineModel = use_clf_pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89156c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d70458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': ['navigate to kitchen'], 'sentence_embeddings': ['navigate to kitchen'], 'class': ['NAV']}\n",
      "['NAV']\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import LightPipeline\n",
    "#clf_pipelineModel = use_clf_pipeline.fit(trainDataset)\n",
    "\n",
    "text = \"navigate to kitchen\"\n",
    "prediction_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "prediction_model = use_clf_pipeline.fit(prediction_data)\n",
    "light = LightPipeline(prediction_model)\n",
    "results = light.annotate(text)\n",
    "print(results)\n",
    "print(results['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c39d6145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAV']\n",
      "['NAV']\n",
      "['VC']\n",
      "['SD']\n",
      "['SD']\n",
      "['VC']\n",
      "['SD']\n",
      "['DA']\n",
      "['DA']\n",
      "['DA']\n",
      "['NAV']\n",
      "['DA']\n",
      "['LR']\n",
      "['AR']\n",
      "['AR']\n",
      "['LR']\n",
      "['LR']\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter Testing Text\\n\")\n",
    "while(text != \"exit\"):\n",
    "    results = light.annotate(text)\n",
    "    print(results['class'])\n",
    "    text = input(\"Enter New Text\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8da94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2243226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f123c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e5299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544efb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b92a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d208a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a21197b13add4b6c4ce7cb24d3cb94b238afe3cb21fedcec323a08881ade42e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
